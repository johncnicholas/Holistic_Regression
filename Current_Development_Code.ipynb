{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Final Project JN - Holistic Regresssion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit to  https://github.com/MichaelLLi/ for implementing this originally in Julia 0.6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: This is a stable final version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ScikitLearn, JuMP, Gurobi, Optim, Distributions, Ipopt, StatsBase,\n",
    "CSV, LinearAlgebra,Rmath, Arpack, Random, DataFrames, StatsPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import preprocessing: normalize\n",
    "@sk_import preprocessing: StandardScaler\n",
    "@sk_import preprocessing: MinMaxScaler\n",
    "@sk_import datasets: make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set paths to data\n",
    "park_path   = \"/Users/johnnicholas/desktop/ML/Hw/Hw3/parkinsons_updrs1.csv\"\n",
    "admit_path  = \"/Users/johnnicholas/desktop/ML/project/code/admit.csv\"\n",
    "boston_path = \"/Users/johnnicholas/desktop/ML/project/code/Boston.csv\"\n",
    "gables_path = \"/Users/johnnicholas/desktop/ML/project/code/CoralGablesHouses.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARKINSONS FROM HW3\n",
    "park = CSV.read(park_path);\n",
    "X_park = park[:, 1:16];\n",
    "y_park = park[:, 17];\n",
    "#ADMIT FROM OLD\n",
    "admit = CSV.read(admit_path)\n",
    "X_admit = admit[:,3:9];\n",
    "y_admit = admit[:,10];\n",
    "#BOSTON DATASET\n",
    "boston = CSV.read(boston_path, header=true)[:,2:15]\n",
    "X_bos = boston[:,1:13];\n",
    "y_bos = boston[:,14];\n",
    "#GABLES HOUSING\n",
    "gables = CSV.read(gables_path, header=true);\n",
    "mid = hcat(gables[:,1],gables[:,7:10]);\n",
    "X_gables = hcat(mid, gables[:,3:6]);\n",
    "y_gables = gables[:,2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function MeanImpute(X)\n",
    "    X_mean_impute = deepcopy(X)\n",
    "    for d in 1:size(X, 2)\n",
    "      idx_missing = ismissing.(X[:,d])\n",
    "      X_mean_impute[idx_missing, d] .= mean(X[.!idx_missing, d])  \n",
    "    end    \n",
    "return X_mean_impute\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function HighCorrelation(X,HC)\n",
    "   X = convert(Matrix, X)\n",
    "corr = cor(X)\n",
    "    z = ones(1,2)\n",
    "for i = 1:size(corr)[1]\n",
    "           for j = i:size(corr)[1]\n",
    "    if abs(corr[i,j]) >= HC && i!=j \n",
    "            q=[i,j]'\n",
    "            z=vcat(z,q)    \n",
    "            end\n",
    "        end\n",
    "    end  \n",
    "  return round.(Int,z[2:size(z)[1],1:2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function betaUB(X,Y,UB,i)\n",
    "    p=size(X)[2]\n",
    "    LB=Model(solver=GurobiSolver(OutputFlag=0))\n",
    "    @variable(LB,β[1:p])\n",
    "    @constraint(LB,norm(Y-X*β)<=UB)\n",
    "    @objective(LB,Max,β[i])\n",
    "    solve(LB)\n",
    "    β_opti=getvalue(β)\n",
    "    return β_opti[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function betaLB(X,Y,UB,i)\n",
    "    p=size(X)[2]\n",
    "    LB=Model(solver=GurobiSolver(OutputFlag=0))\n",
    "    @variable(LB,β[1:p])\n",
    "    @constraint(LB,norm(Y-X*β)<=UB)\n",
    "    @objective(LB,Min,β[i])\n",
    "    solve(LB)\n",
    "    β_opti=getvalue(β)\n",
    "    return β_opti[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function findobjUB(X,Y)\n",
    "p=size(X)[2]\n",
    "    function naiveobjective(β)\n",
    "        obj=norm(Y-X*β,2)\n",
    "        return obj\n",
    "    end\n",
    "initial_x=Array{Float64}(undef, p)\n",
    "initial_x.=0\n",
    "res=Optim.optimize(naiveobjective, initial_x, NelderMead(),Optim.Options(time_limit = 10.0))\n",
    "obj_UB=Optim.minimum(res)\n",
    "β_UB=Optim.minimizer(res)\n",
    "return obj_UB, β_UB\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function findM(X,Y)\n",
    "    Y= convert(Array, Y)\n",
    "    X = convert(Matrix, X)\n",
    "    obj_UB, β_UB=findobjUB(X,Y)\n",
    "    println(\"Upper bound of Objective is $obj_UB\")\n",
    "    p=size(X)[2]\n",
    "    UBArray=Array{Float64}(undef, p)\n",
    "    LBArray=Array{Float64}(undef, p)\n",
    "    for i=1:p\n",
    "        UBArray[i]=betaUB(X,Y,obj_UB,i)\n",
    "        LBArray[i]=betaLB(X,Y,obj_UB,i)\n",
    "    end\n",
    "    return max(maximum(UBArray),minimum(LBArray))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function NLTransform(X)\n",
    "    X = convert(Matrix, X)\n",
    "    logX=log.(X)\n",
    "    sqrtX=sqrt.(X)\n",
    "    sqX=X.*X\n",
    "    oldColumn=size(X)[2]\n",
    "    X=hcat(X,logX,sqrtX,sqX)\n",
    "    NLArray=Matrix{Int64}(I,oldColumn,4)\n",
    "    for i=1:oldColumn\n",
    "            NLArray[i,1]=i\n",
    "            NLArray[i,2]=i+oldColumn\n",
    "            NLArray[i,3]=i+oldColumn*2\n",
    "            NLArray[i,4]=i+oldColumn*3\n",
    "    end\n",
    "    X = replace!(X, -Inf=>0)\n",
    "    X = replace!(X, Inf=>0)\n",
    "    return NLArray, X\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function GlobalMCor(x,delta)    \n",
    "    X = convert(Matrix, x)\n",
    "    cols = eigvals(X'*X).<= 0.01\n",
    "    v = eigvecs(X'*X)[:,cols]\n",
    "    p = size(v)[1] \n",
    "    m = size(v)[2] \n",
    "    partd = Model(solver=GurobiSolver(TimeLimit=45, OutputFlag=0))\n",
    "    # Optimization variables\n",
    "    @variable(partd, z[j=1:p], Bin)\n",
    "    @variable(partd, d[i=1:m], Bin)\n",
    "    @variable(partd, y[h=1:2], Bin)\n",
    "    @variable(partd, a[j=1:p])\n",
    "    @variable(partd, t[i=1:m])\n",
    "    @variable(partd, b)\n",
    "    # Objective\n",
    "    @objective(partd, Min, sum(z))\n",
    "    # Constraints\n",
    "    @constraint(partd, [j=1:p], 500*z[j] >= a[j])\n",
    "    @constraint(partd, [j=1:p], -500*z[j] <= a[j])\n",
    "    @constraint(partd, sum(t[i].*v[:,i] for i in 1:m, dims=2) .== a)\n",
    "    @constraint(partd, b == sum(t))\n",
    "    @constraint(partd, b <= -delta + (500)*(1-y[1]))\n",
    "    @constraint(partd, b >= delta + (-500)*(1-y[2]))\n",
    "    @constraint(partd, y[1] + y[2] == 1)\n",
    "    solve(partd)\n",
    "    status = solve(partd)\n",
    "S=[]    \n",
    "while status != \"INFEASIBLE\"\n",
    "supp_a = findall(x -> x >= 0.001, getvalue(z))\n",
    "        \n",
    "  if iszero(supp_a) == false\n",
    "    push!(S, supp_a)\n",
    "    @constraint(partd, sum(z[i] for i in supp_a) <= length(supp_a)-1)\n",
    "    solve(partd)\n",
    "    status = solve(partd) \n",
    "  else \n",
    "    break\n",
    "  end      \n",
    "end\n",
    "    if size(S,1)<=1\n",
    "        S=zeros(1,(size(X,2)))\n",
    "    else\n",
    "    end\n",
    "S = filter(x -> size(x,1) >=2 ,S)   \n",
    " return S   \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Normalize(X, Y)\n",
    "scaler =  MinMaxScaler()\n",
    "        X = ScikitLearn.fit_transform!(scaler,X)\n",
    "        Y = ScikitLearn.fit_transform!(scaler,convert(Matrix,hcat(Y,ones(size(Y,1),1))))[:,1]\n",
    "    return X,Y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function OptSplit(vector,trainp)\n",
    "model = Model(solver=GurobiSolver(TimeLimit=120, OutputFlag=0))\n",
    "    full_size = size(vector,1)\n",
    "    train_size=round(Int,size(vector,1)*trainp)\n",
    "    one1 = vector\n",
    "    rho=1\n",
    "    # Optimization variables\n",
    "    h = size(one1,1)\n",
    "    @variable(model, x[i=1:h,p=1:2], Bin)\n",
    "    @variable(model, d>=0)\n",
    "    \n",
    "    # Objective\n",
    "    @objective(model, Min, d)\n",
    "    \n",
    "    \n",
    "    # Constraints\n",
    "    #u1 - u2 + o1 - o2\n",
    "    @constraint(model, d >= (1/10)*sum(one1[i]*x[i,1] for i in 1:h) - (1/10)*sum(one1[i]*x[i,2] for i in 1:h)\n",
    "                       +rho*(1/10)*sum((one1[i]^2)*x[i,1] for i in 1:h) - rho*(1/10)*sum((one1[i]^2)*x[i,2] for i in 1:h))\n",
    "\n",
    "    #u1 - u2 - o1 + o2\n",
    "    @constraint(model, d >= (1/10)*sum(one1[i]*x[i,1] for i in 1:h) - (1/10)*sum(one1[i]*x[i,2] for i in 1:h)\n",
    "                        -rho*(1/10)*sum((one1[i]^2)*x[i,1] for i in 1:h) + rho*(1/10)*sum((one1[i]^2)*x[i,2] for i in 1:h))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #-u1 + u2 + o1 - o2\n",
    "     @constraint(model, d >= -(1/10)*sum(one1[i]*x[i,1] for i in 1:h)+(1/10)*sum(one1[i]*x[i,2] for i in 1:h)\n",
    "                          +rho*(1/10)*sum((one1[i]^2)*x[i,1] for i in 1:h) - rho*(1/10)*sum((one1[i]^2)*x[i,2] for i in 1:h)) \n",
    "\n",
    "\n",
    "    #-u1 + u2 - o1 + o2\n",
    "    @constraint(model, d >= -(1/10)*sum(one1[i]*x[i,1] for i in 1:h)+(1/10)*sum(one1[i]*x[i,2] for i in 1:h)\n",
    "                        -rho*(1/10)*sum((one1[i]^2)*x[i,1] for i in 1:h) + rho*(1/10)*sum((one1[i]^2)*x[i,2] for i in 1:h))\n",
    "    \n",
    "@constraint(model, sum(x[i,1] for i in 1:h) == train_size)\n",
    "@constraint(model, sum(x[i,2] for i in 1:h) == abs(train_size-full_size))\n",
    "@constraint(model, [i=1:h], sum(x[i,p] for p in 1:2) == 1)   \n",
    "     solve(model)\n",
    "\n",
    "  return getvalue(d), getvalue(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function OptSplitWrapper(X,vector, trainp)\n",
    "    full_size = size(vector,1)\n",
    "    y = OptSplit(vector,trainp)\n",
    "    first_grp  = filter( y -> abs(y)>=0.000000001, (round.(Int,y[2][1:full_size]).*vector));\n",
    "    second_grp = filter( y -> abs(y)>=0.000000001, (round.(Int,y[2][full_size+1:2*full_size]).*vector));\n",
    "\n",
    "    #Recover X_train from OptSplit y_train\n",
    "    admit_c = convert(DataFrame,hcat(convert(Matrix,X),convert(Vector,vector)))\n",
    "    X_c = convert(DataFrame,copy(X))\n",
    "    X_group1 = ones(1,size(X,2))\n",
    "    for i in 1:size(first_grp,1)\n",
    "        j = findall(x-> x==first_grp[i],admit_c[:,size(admit_c,2)])[1]\n",
    "        X_group1 = vcat(X_group1 , convert(Vector,X_c[j,:])')\n",
    "        deleterows!(admit_c, j)\n",
    "        deleterows!(X_c, j)\n",
    "    end\n",
    "    X_train = X_group1[2:size(X_group1,1),:]\n",
    "    #END\n",
    "    \n",
    "    #Recover X_test from OptSplit y_train\n",
    "    X_group2 = ones(1,size(X,2))\n",
    "    for i in 1:size(second_grp,1)\n",
    "        j = findall(x-> x==second_grp[i],admit_c[:,size(admit_c,2)])[1]\n",
    "        X_group2 = vcat(X_group2 , convert(Vector,X_c[j,:])')\n",
    "        deleterows!(admit_c, j)\n",
    "        deleterows!(X_c, j)\n",
    "    end\n",
    "    X_test = X_group2[2:size(X_group2,1),:]\n",
    "    #END\n",
    "    \n",
    "    return first_grp, second_grp, X_train, X_test\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now comes regression methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function OptimRegression(X,Y,X_valid,Y_valid,X_test,Y_test,invX,vari,HCArray,HC,M,NLArray,sign,MCArray,NLtrue=false, Signtrue=false,returnbeta=false, MCtrue=false)\n",
    "    function RegressionWrapper(x)\n",
    "        obj, β=SystematicRegression(X,Y,invX,vari,x[1],x[2],HCArray,HC,M,NLArray,sign,MCArray,NLtrue,Signtrue,true, MCtrue)\n",
    "        β[findall(abs.(β).<1e-5)].=0\n",
    "        obj_valid=norm(Y_valid-X_valid*β,2)\n",
    "        return obj_valid\n",
    "    end\n",
    "    initial_x=[0.1,5]\n",
    "    println(\"Optimizer Starting\")\n",
    "    res=Optim.optimize(RegressionWrapper, initial_x, NelderMead(),Optim.Options(f_calls_limit=20))\n",
    "    opti_HP=Optim.minimizer(res)\n",
    "    obj_valid=Optim.minimum(res)\n",
    "    obj_opti,β_opti=SystematicRegression(X,Y,invX,vari,opti_HP[1],opti_HP[2],HCArray,HC,M,NLArray,sign,MCArray,NLtrue,Signtrue,true,MCtrue)\n",
    "    β_opti[findall(abs.(β_opti).<1e-5)].=0\n",
    "    obj_test=norm(Y_test-X_test*β_opti,2)\n",
    "    return β_opti,obj_valid,obj_test,opti_HP[1],opti_HP[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SystematicRegression(X,Y,invX,vari,Γ,k,HCArray,HC,M,NLArray,sign,MCArray,NLtrue=false, Signtrue=false,returnbeta=false,MCtrue=false)\n",
    "# If we want to also test significance, write Signtrue=true\n",
    "# sign is the t-value threshold you want to test\n",
    "# if you want beta values to be returned, take returnbeta as true\n",
    "if Γ<=0\n",
    "    Γ=0\n",
    "end\n",
    "if Γ>=1\n",
    "    Γ=1\n",
    "end\n",
    "if k>=size(X)[2]-size(NLArray)[1]*3\n",
    "    k=size(X)[2]-size(NLArray)[1]*3\n",
    "end\n",
    "if k<=1\n",
    "    k=1\n",
    "end\n",
    "M2=100000\n",
    "model=Model(solver=GurobiSolver(OutputFlag=0,NumericFocus=2))\n",
    "if size(X)[1] !=size(Y)[1]\n",
    "    throw(ArgumentError(\"check sizes of array X and Y\"))\n",
    "end\n",
    "n=size(X)[1]\n",
    "p=size(X)[2]\n",
    "@variable(model,z[1:p],Bin)\n",
    "@variable(model,β[1:p])\n",
    "@variable(model,l[1:p])\n",
    "@constraint(model,[i=1:p], l[i]>=β[i])\n",
    "@constraint(model,[i=1:p], l[i]>=-β[i])\n",
    "@constraint(model,[i=1:p], M*z[i]>=β[i])\n",
    "@constraint(model,[i=1:p], M*z[i]>=-β[i])\n",
    "@constraint(model,sum(z[i] for i=1:p)<=k) #Sparsity\n",
    "if size(HCArray)[1]>0\n",
    "    for i=1:size(HCArray)[1]\n",
    "        @constraint(model,sum(z[HCArray[i,j]] for j=1:size(HCArray)[2])<=1) #pairwise collinearity\n",
    "    end\n",
    "end\n",
    "if NLtrue\n",
    "    for i=1:size(NLArray)[1]\n",
    "        @constraint(model,sum(z[NLArray[i,j]] for j=1:size(NLArray)[2])<=1) # Nonlinear transformation\n",
    "    end\n",
    "end\n",
    "if Signtrue\n",
    "    if NLtrue\n",
    "        @variable(model,b[1:p],Bin)\n",
    "        for i=1:p\n",
    "            k=mod(i,p/4)\n",
    "            if k==0\n",
    "                k=p/4\n",
    "            end\n",
    "            k=convert(Int64,k)\n",
    "            c=(1)./sqrt(vari*invX[k,k])\n",
    "            @constraint(model,β[i]*c[1]+M2*c[1]*b[i]>=(quantile(TDist(n-p),1-sign))*z[i]) #significance\n",
    "            @constraint(model,β[i]*c[1]+M2*c[1]*b[i]<=M2*c[1]-(quantile(TDist(n-p),1-sign))*z[i])  #significance\n",
    "        end\n",
    "    else\n",
    "        @variable(model,b[1:p],Bin)\n",
    "        for i=1:p\n",
    "            c=(1)./sqrt(vari*invX[i,i])\n",
    "            @constraint(model,β[i]*c[1]+M2*c[1]*b[i]>=(quantile.(TDist(n-p),1-sign*0.5))*z[i]) #significance\n",
    "            @constraint(model,β[i]*c[1]+M2*c[1]*b[i]<=M2*c[1]-(quantile.(TDist(n-p),1-sign*0.5))*z[i]) #significance\n",
    "        end\n",
    "    end\n",
    "end\n",
    "if MCtrue\n",
    "      if size(MCArray,1)>0                     \n",
    "       for i in 1:size(MCArray,1)\n",
    "        @constraint(model,sum(z[MCArray[i][j]] for j in 1:size(MCArray[i],1)) <= (size(MCArray[i],1)))\n",
    "       end\n",
    "      end\n",
    "end\n",
    "@objective(model, Min, sum((Y[i]-(X*β)[i])*(Y[i]-(X*β)[i]) for i=1:n)/2+sum(l[i] for i=1:p)Γ)\n",
    "solve(model)\n",
    "β_opti=getvalue(β)\n",
    "objective=getobjectivevalue(model)\n",
    "if returnbeta\n",
    "    return objective, β_opti\n",
    "else\n",
    "    return objective\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SRegression2(X,Y,HC,NLtrue,sign,Signtrue,Normtrue,MCtrue,Imputetrue) \n",
    "      #Normalization/Imputation\n",
    "    if Normtrue \n",
    "       nrm = Normalize(X,Y)\n",
    "        X = nrm[1]\n",
    "        Y = nrm[2]\n",
    "    end\n",
    "     println(\"Normalization complete\")\n",
    "     \n",
    "    if Imputetrue\n",
    "    X = MeanImpute(X)\n",
    "    end\n",
    "   \n",
    "    \n",
    "    #There is no random/optimal splitting done in here, just splitting up what's passed in.\n",
    "    train=round(Int,size(X)[1]*0.6)\n",
    "    test=round(Int,size(X)[1]*0.8)\n",
    "     \n",
    "  \n",
    "    X_train_1=X[1:train,:]\n",
    "    Y_train_1=Y[1:train]\n",
    "    \n",
    "    M=findM(X_train_1,Y_train_1)\n",
    "    println(\"M found: $M\")\n",
    "    invX=inv(X_train_1'X_train_1)\n",
    "    P=X_train_1*(invX)X_train_1'\n",
    "    n=size(X_train_1)[1]\n",
    "    p=size(X_train_1)[2]\n",
    "    RSS=Y_train_1'*(I(n)-P)Y_train_1\n",
    "    vari=RSS/(n-p)\n",
    "    println(\"Significance Calculated\")\n",
    "    if NLtrue && !Normtrue\n",
    "        println(\"ERROR if NLTRUE, THEN ALSO MAKE NORM TRUE\")\n",
    "    elseif NLtrue\n",
    "        NLArray, X=NLTransform(X)\n",
    "    else\n",
    "        NLArray = zeros(0,0)\n",
    "    end\n",
    "    X_train=X[1:train,:]\n",
    "    Y_train=Y[1:train]\n",
    "    X_valid=X[train+1:test,:]\n",
    "    Y_valid=Y[train+1:test]\n",
    "    X_test=X[test:end,:]\n",
    "    Y_test=Y[test:end]\n",
    "    println(\"Split completed\")\n",
    "    HCArray=HighCorrelation(X_train,HC)\n",
    "    println(\"HCArray found\")\n",
    "    MCArray = GlobalMCor(X_train,0.015)\n",
    "    MCArraySize=size(MCArray,1)\n",
    "    println(\"MCArray found, Size: $MCArraySize\")\n",
    "    println(MCArray)\n",
    "    β, obj_valid,obj_test, Γ, k=OptimRegression(X_train,Y_train,X_valid,Y_valid,X_test,Y_test,invX,vari,HCArray,HC,M,NLArray,sign,MCArray,NLtrue,Signtrue,false,MCtrue)\n",
    "    return β, obj_valid,obj_test, Γ, k\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now wrote Holistic Regression Wrapper that just takes data & T/F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function HolisticRegression(X,Y,HC=0.99,NLtrue=false,sign=0.95,Signtrue=false,Normtrue=false,\n",
    "                             MCtrue=false,OptSplit=false,Imputetrue=false,trainp=0.8)\n",
    "    \n",
    "    #Conversion to array if not an array\n",
    "    if typeof(X) != Array\n",
    "      X=convert(Matrix{Float64},X)\n",
    "    end\n",
    "    if typeof(X) != Array\n",
    "      Y=convert(Vector{Float64},Y)\n",
    "    end\n",
    "    #End\n",
    "    \n",
    "    #Split into train and test\n",
    "    if OptSplit == true\n",
    "        groups = OptSplitWrapper(X,Y, trainp)\n",
    "        Y_train = groups[1] \n",
    "        Y_test = groups[2]\n",
    "        X_train = groups[3]\n",
    "        X_test = groups[4]\n",
    "    println(\"Optimal data split calculated\")\n",
    "          \n",
    "    else\n",
    "      Random.seed!(4567)\n",
    "      index = Distributions.sample(1:size(X,1),size(X,1),replace=false);\n",
    "      train=round(Int,size(X,1)*trainp)\n",
    "      X_train = X[1:train,:]\n",
    "      X_test =  X[train+1:size(X,1),:]\n",
    "      Y_train = Y[1:train,:]\n",
    "      Y_test =  Y[train+1:size(X,1),:]\n",
    "    end\n",
    "    \n",
    "    if NLtrue == true\n",
    "    results = SRegression2(X_train,Y_train,HC,NLtrue,sign,Signtrue,Normtrue,MCtrue,Imputetrue)\n",
    "    #IN SAMPLE RESULTS\n",
    "        scaler =  MinMaxScaler()\n",
    "        x = NLTransform(ScikitLearn.fit_transform!(scaler,X_train))\n",
    "    #x = NLTransform(normalize(X_train))\n",
    "    y_hat = x[2]*results[1];\n",
    "     y = ScikitLearn.fit_transform!(scaler,convert(Matrix,hcat(Y_train,ones(size(Y_train,1),1))))[:,1]\n",
    "    #y = normalize(convert(Matrix,hcat(Y_train,ones(size(Y_train,1),1))))[:,1]\n",
    "     #y = convert(Array,Y_train)\n",
    "    SSE1 = sum((y_hat.-y).^2)\n",
    "    SST = sum((mean(y).-y).^2)\n",
    "    In_Sample_Rsq = (1-(SSE1/SST))\n",
    "    In_Sample_RMSE = sqrt(mean(((y_hat).-y).^2))\n",
    "    #OUT OF SAMPLE RESULTS\n",
    "    x = NLTransform(ScikitLearn.fit_transform!(scaler,X_test))\n",
    "    #x = NLTransform(normalize(X_test))\n",
    "    #x = NLTransform(X_test)\n",
    "    y_hat = x[2]*results[1];\n",
    "      #y = convert(Array,Y_test)\n",
    "    #y = normalize(convert(Matrix,hcat(Y_test,ones(size(Y_test,1),1))))[:,1]\n",
    "    y = ScikitLearn.fit_transform!(scaler,convert(Matrix,hcat(Y_test,ones(size(Y_test,1),1))))[:,1]\n",
    "    SSE1 = sum((y_hat.-y).^2)\n",
    "    SST = sum((mean(y).-y).^2)\n",
    "    Out_Sample_Rsq = (1-(SSE1/SST))\n",
    "    Out_Sample_RMSE = sqrt(mean(((y_hat).-y).^2))\n",
    "    results = results[1]\n",
    "    \n",
    "    else\n",
    "    results = SRegression2(X_train,Y_train,HC,NLtrue,sign,Signtrue,Normtrue,MCtrue,Imputetrue)\n",
    "    #IN SAMPLE RESULTS\n",
    "    x = convert(Matrix,X_train)\n",
    "    y_hat = x*results[1];\n",
    "    y = convert(Array, Y_train)\n",
    "    SSE1 = sum((y_hat.-y).^2)\n",
    "    SST = sum((mean(y).-y).^2)\n",
    "    In_Sample_Rsq = (1-(SSE1/SST))\n",
    "    In_Sample_RMSE = sqrt(mean(((y_hat).-y).^2))\n",
    "    #OUT OF SAMPLE RESULTS\n",
    "    xt = convert(Matrix,X_test)\n",
    "    y_hat = xt*results[1];\n",
    "    y = convert(Array, Y_test)\n",
    "    SSE2 = sum((y_hat.-y).^2)\n",
    "    SST = sum((mean(y).-y).^2)\n",
    "    Out_Sample_Rsq = (1-(SSE2/SST))\n",
    "    Out_Sample_RMSE = sqrt(mean(((y_hat).-y).^2))   \n",
    "    end\n",
    "\n",
    "println(\"In sample R^2: $In_Sample_Rsq\")\n",
    "println(\"Out of sample R^2: $Out_Sample_Rsq\")\n",
    "println(\"In sample RMSE: $In_Sample_RMSE\")\n",
    "println(\"Out of sample RMSE: $Out_Sample_RMSE \")\n",
    "return results\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X,Y,HC=0.99,NLtrue=false,sign=0.95,Signtrue=false,Normtrue=false,\n",
    "                             #MCtrue=false,OptSplit=false,Imputetrue=false,trainp=0.8)\n",
    "HolisticRegression(X_admit, y_admit,0.99, true, 0.95, true, true, true, false, false, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X,Y,HC=0.99,NLtrue=false,sign=0.95,Signtrue=false,Normtrue=false,\n",
    "                             #MCtrue=false,OptSplit=false,Imputetrue=false,trainp=0.8)\n",
    "HolisticRegression(X_gables, y_gables, 0.99, true, 0.95, true, true, true, true,false,0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
